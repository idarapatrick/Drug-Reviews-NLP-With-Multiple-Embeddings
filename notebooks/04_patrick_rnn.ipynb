{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RNN Models for Drug Reviews Classification with Multiple Embeddings\n",
        "\n",
        "**Team Member:** Patrick  \n",
        "**Model:** RNN (Recurrent Neural Network / SimpleRNN)  \n",
        "**Embeddings:** TF-IDF, Word2Vec Skip-gram, Word2Vec CBOW\n",
        "\n",
        "---\n",
        "\n",
        "## Objectives\n",
        "1. Implement RNN architecture for binary drug review sentiment classification\n",
        "2. Train RNN with at least 3 different embedding techniques (TF-IDF, Skip-gram, CBOW)\n",
        "3. Perform systematic hyperparameter tuning for optimal performance\n",
        "4. Document all experiments with academic rigor\n",
        "5. Compare embedding performance and save results for team analysis\n",
        "\n",
        "**Research Question:** How do different word embedding techniques (TF-IDF vs. Word2Vec Skip-gram vs. Word2Vec CBOW) impact the performance of RNN models for drug review sentiment classification?\n",
        "\n",
        "**Dataset:** Drug Reviews from UCI Machine Learning Repository (Drugs.com)\n",
        "- **Task:** Binary Sentiment Classification\n",
        "- **Positive:** Rating >= 7\n",
        "- **Negative:** Rating <= 4\n",
        "- **Neutral:** Rating 5-6 (excluded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Embedding, Dropout, Input, Reshape\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Add project root to path\n",
        "notebook_dir = Path('.').resolve()\n",
        "project_root = notebook_dir.parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import custom modules\n",
        "from src.preprocessing_pipeline import prepare_datasets\n",
        "from embeddings.word2vec_embedding import Word2VecEmbedding, get_word2vec_embedding\n",
        "from embeddings.tfidf_embedding import TfidfEmbedding\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "tf.random.set_seed(RANDOM_STATE)\n",
        "\n",
        "print(f'‚úì TensorFlow version: {tf.__version__}')\n",
        "print(f'‚úì GPU available: {len(tf.config.list_physical_devices(\"GPU\")) > 0}')\n",
        "print(f'‚úì Project root: {project_root}')\n",
        "print('‚úì Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data paths\n",
        "DATA_PATH_TRAIN = str(project_root / 'data' / 'drug_review_train.csv')\n",
        "DATA_PATH_VAL = str(project_root / 'data' / 'drug_review_validation.csv')\n",
        "DATA_PATH_TEST = str(project_root / 'data' / 'drug_review_test.csv')\n",
        "\n",
        "# Model hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 0.001\n",
        "RNN_UNITS_1 = 64  # First RNN layer units\n",
        "RNN_UNITS_2 = 32  # Second RNN layer units\n",
        "DROPOUT_RATE = 0.3\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "REDUCE_LR_PATIENCE = 5\n",
        "\n",
        "# Embedding configurations\n",
        "EMBEDDING_DIMS = {\n",
        "    'word2vec_skipgram': 200,  # Using medium config\n",
        "    'word2vec_cbow': 200,       # Using medium config\n",
        "    'tfidf': 2000               # From preprocessing pipeline\n",
        "}\n",
        "\n",
        "# Word2Vec training parameters\n",
        "W2V_WINDOW_SIZE = 8\n",
        "W2V_MIN_COUNT = 2\n",
        "W2V_EPOCHS = 10\n",
        "W2V_WORKERS = 4\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Max epochs: {EPOCHS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  RNN units: {RNN_UNITS_1} -> {RNN_UNITS_2}\")\n",
        "print(f\"  Dropout rate: {DROPOUT_RATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets using preprocessing pipeline\n",
        "print(\"Loading datasets from preprocessing pipeline...\")\n",
        "\n",
        "data = prepare_datasets(\n",
        "    train_path=DATA_PATH_TRAIN,\n",
        "    val_path=DATA_PATH_VAL,\n",
        "    test_path=DATA_PATH_TEST\n",
        ")\n",
        "\n",
        "# Unpack datasets\n",
        "X_seq_train, X_tfidf_train, y_train = data['train']\n",
        "X_seq_val, X_tfidf_val, y_val = data['val']\n",
        "X_seq_test, X_tfidf_test, y_test = data['test']\n",
        "vocab_size = data['vocab_size']\n",
        "tokenizer = data['tokenizer']\n",
        "tfidf_vectorizer = data['tfidf']\n",
        "\n",
        "print(f\"\\nDataset shapes:\")\n",
        "print(f\"  Train: X_seq={X_seq_train.shape}, X_tfidf={X_tfidf_train.shape}, y={y_train.shape}\")\n",
        "print(f\"  Val:   X_seq={X_seq_val.shape}, X_tfidf={X_tfidf_val.shape}, y={y_val.shape}\")\n",
        "print(f\"  Test:  X_seq={X_seq_test.shape}, X_tfidf={X_tfidf_test.shape}, y={y_test.shape}\")\n",
        "\n",
        "print(f\"\\nSentiment distribution:\")\n",
        "print(f\"  Train: Negative={np.sum(y_train==0)}, Positive={np.sum(y_train==1)}\")\n",
        "print(f\"  Val:   Negative={np.sum(y_val==0)}, Positive={np.sum(y_val==1)}\")\n",
        "print(f\"  Test:  Negative={np.sum(y_test==0)}, Positive={np.sum(y_test==1)}\")\n",
        "\n",
        "print(f\"\\nVocabulary size: {vocab_size}\")\n",
        "print(f\"Sequence length: {X_seq_train.shape[1]}\")\n",
        "print(f\"TF-IDF features: {X_tfidf_train.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Embeddings\n",
        "\n",
        "### 4.1 TF-IDF Embedding\n",
        "TF-IDF vectors are already prepared by the preprocessing pipeline. We'll use `X_tfidf_train`, `X_tfidf_val`, and `X_tfidf_test` directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF is already prepared\n",
        "print(\"‚úì TF-IDF embeddings ready\")\n",
        "print(f\"  Train shape: {X_tfidf_train.shape}\")\n",
        "print(f\"  Features: {X_tfidf_train.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Word2Vec Skip-gram Embedding\n",
        "\n",
        "Skip-gram learns word representations by predicting context words from a target word. This approach captures semantic relationships and typically performs better on rare words compared to CBOW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare tokenized texts for Word2Vec training\n",
        "# Load original texts and tokenize them for Word2Vec\n",
        "print(\"Preparing tokenized texts for Word2Vec Skip-gram training...\")\n",
        "\n",
        "# Load original training data\n",
        "df_train = pd.read_csv(DATA_PATH_TRAIN, index_col=0)\n",
        "df_train = df_train[(df_train['rating'] <= 4) | (df_train['rating'] >= 7)].copy()\n",
        "df_train['review'] = df_train['review'].astype(str).fillna('')\n",
        "\n",
        "# Tokenize texts into word lists for Word2Vec\n",
        "# Simple tokenization: split by whitespace and convert to lowercase\n",
        "train_tokens = [text.lower().split() for text in df_train['review']]\n",
        "\n",
        "print(f\"‚úì Prepared {len(train_tokens)} training token sequences\")\n",
        "print(f\"  Sample tokens: {train_tokens[0][:10]}\")\n",
        "\n",
        "# Train Word2Vec Skip-gram model\n",
        "print(\"\\nTraining Word2Vec Skip-gram model...\")\n",
        "w2v_skipgram = get_word2vec_embedding('skipgram_medium')\n",
        "w2v_skipgram.fit(train_tokens)\n",
        "\n",
        "print(f\"‚úì Skip-gram vocabulary size: {w2v_skipgram.get_vocabulary_size()}\")\n",
        "print(f\"‚úì Embedding dimension: {w2v_skipgram.embedding_dim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create embedding matrix for Skip-gram\n",
        "embedding_dim_skipgram = w2v_skipgram.embedding_dim\n",
        "embedding_matrix_skipgram = np.zeros((vocab_size, embedding_dim_skipgram))\n",
        "\n",
        "words_found = 0\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "    if idx < vocab_size:\n",
        "        try:\n",
        "            embedding_matrix_skipgram[idx] = w2v_skipgram.get_word_vector(word)\n",
        "            words_found += 1\n",
        "        except KeyError:\n",
        "            # Word not in Word2Vec vocabulary - initialize randomly\n",
        "            embedding_matrix_skipgram[idx] = np.random.normal(0, 0.01, embedding_dim_skipgram)\n",
        "\n",
        "print(f\"‚úì Embedding matrix shape: {embedding_matrix_skipgram.shape}\")\n",
        "print(f\"‚úì Words found in Word2Vec: {words_found}/{vocab_size}\")\n",
        "print(f\"‚úì Coverage: {words_found/vocab_size*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Word2Vec CBOW Embedding\n",
        "\n",
        "CBOW (Continuous Bag of Words) predicts a target word from its context. It's faster to train than Skip-gram and often works better for frequent words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Word2Vec CBOW model\n",
        "print(\"Training Word2Vec CBOW model...\")\n",
        "w2v_cbow = get_word2vec_embedding('cbow_medium')\n",
        "w2v_cbow.fit(train_tokens)\n",
        "\n",
        "print(f\"‚úì CBOW vocabulary size: {w2v_cbow.get_vocabulary_size()}\")\n",
        "print(f\"‚úì Embedding dimension: {w2v_cbow.embedding_dim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create embedding matrix for CBOW\n",
        "embedding_dim_cbow = w2v_cbow.embedding_dim\n",
        "embedding_matrix_cbow = np.zeros((vocab_size, embedding_dim_cbow))\n",
        "\n",
        "words_found_cbow = 0\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "    if idx < vocab_size:\n",
        "        try:\n",
        "            embedding_matrix_cbow[idx] = w2v_cbow.get_word_vector(word)\n",
        "            words_found_cbow += 1\n",
        "        except KeyError:\n",
        "            # Word not in Word2Vec vocabulary - initialize randomly\n",
        "            embedding_matrix_cbow[idx] = np.random.normal(0, 0.01, embedding_dim_cbow)\n",
        "\n",
        "print(f\"‚úì Embedding matrix shape: {embedding_matrix_cbow.shape}\")\n",
        "print(f\"‚úì Words found in Word2Vec: {words_found_cbow}/{vocab_size}\")\n",
        "print(f\"‚úì Coverage: {words_found_cbow/vocab_size*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Architecture Definitions\n",
        "\n",
        "### 5.1 RNN with TF-IDF\n",
        "\n",
        "For TF-IDF, we reshape the dense vectors to sequence format suitable for RNN processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_rnn_tfidf(tfidf_dim=2000, rnn_units_1=64, rnn_units_2=32, dropout_rate=0.3, name=\"RNN_TFIDF\"):\n",
        "    \"\"\"\n",
        "    Build RNN model for TF-IDF input.\n",
        "    \n",
        "    Since TF-IDF is a fixed-length vector, we reduce dimensions and reshape for RNN processing.\n",
        "    \n",
        "    Args:\n",
        "        tfidf_dim: Dimension of TF-IDF vectors (2000)\n",
        "        rnn_units_1: Number of units in first RNN layer\n",
        "        rnn_units_2: Number of units in second RNN layer\n",
        "        dropout_rate: Dropout rate\n",
        "        name: Model name\n",
        "    \n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    # Input: TF-IDF vector (2000 features)\n",
        "    tfidf_input = Input(shape=(tfidf_dim,), name='tfidf_input')\n",
        "    \n",
        "    # Reduce dimensions and prepare for RNN\n",
        "    dense_reduce = Dropout(dropout_rate)(tfidf_input)\n",
        "    dense_reduce = Dense(100, activation='relu', name='dense_reduce')(dense_reduce)\n",
        "    \n",
        "    # Reshape to sequence format (batch_size, timesteps=1, features=100)\n",
        "    reshaped = Reshape((1, 100), name='reshape')(dense_reduce)\n",
        "    \n",
        "    # RNN layers\n",
        "    rnn_1 = SimpleRNN(rnn_units_1, activation='tanh', return_sequences=True, name='rnn_1')(reshaped)\n",
        "    rnn_1 = Dropout(dropout_rate, name='rnn1_dropout')(rnn_1)\n",
        "    \n",
        "    rnn_2 = SimpleRNN(rnn_units_2, activation='tanh', name='rnn_2')(rnn_1)\n",
        "    rnn_2 = Dropout(dropout_rate, name='rnn2_dropout')(rnn_2)\n",
        "    \n",
        "    # Dense layers\n",
        "    dense = Dense(32, activation='relu', name='dense')(rnn_2)\n",
        "    dense = Dropout(dropout_rate, name='dense_dropout')(dense)\n",
        "    \n",
        "    # Output layer\n",
        "    output = Dense(1, activation='sigmoid', name='output')(dense)\n",
        "    \n",
        "    model = Model(inputs=tfidf_input, outputs=output, name=name)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', 'precision', 'recall']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build TF-IDF model\n",
        "rnn_tfidf = build_rnn_tfidf(\n",
        "    tfidf_dim=X_tfidf_train.shape[1],\n",
        "    rnn_units_1=RNN_UNITS_1,\n",
        "    rnn_units_2=RNN_UNITS_2,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    name=\"RNN_TFIDF\"\n",
        ")\n",
        "\n",
        "print(\"RNN MODEL: TF-IDF Embedding\")\n",
        "rnn_tfidf.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 RNN with Word2Vec Skip-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_rnn_word2vec(vocab_size, embedding_dim, embedding_matrix, rnn_units_1=64, rnn_units_2=32, \n",
        "                       dropout_rate=0.3, trainable_embeddings=False, name=\"RNN_Word2Vec\"):\n",
        "    \"\"\"\n",
        "    Build RNN model with Word2Vec embeddings.\n",
        "    \n",
        "    Args:\n",
        "        vocab_size: Vocabulary size\n",
        "        embedding_dim: Embedding dimension\n",
        "        embedding_matrix: Pre-trained embedding matrix\n",
        "        rnn_units_1: Number of units in first RNN layer\n",
        "        rnn_units_2: Number of units in second RNN layer\n",
        "        dropout_rate: Dropout rate\n",
        "        trainable_embeddings: Whether to fine-tune embeddings during training\n",
        "        name: Model name\n",
        "    \n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            weights=[embedding_matrix],\n",
        "            input_length=X_seq_train.shape[1],\n",
        "            trainable=trainable_embeddings,\n",
        "            name='embedding'\n",
        "        ),\n",
        "        SimpleRNN(rnn_units_1, activation='tanh', return_sequences=True, name='rnn_1'),\n",
        "        Dropout(dropout_rate, name='rnn1_dropout'),\n",
        "        SimpleRNN(rnn_units_2, activation='tanh', name='rnn_2'),\n",
        "        Dropout(dropout_rate, name='rnn2_dropout'),\n",
        "        Dense(32, activation='relu', name='dense'),\n",
        "        Dropout(dropout_rate, name='dense_dropout'),\n",
        "        Dense(1, activation='sigmoid', name='output')\n",
        "    ], name=name)\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', 'precision', 'recall']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build Skip-gram model\n",
        "rnn_skipgram = build_rnn_word2vec(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim_skipgram,\n",
        "    embedding_matrix=embedding_matrix_skipgram,\n",
        "    rnn_units_1=RNN_UNITS_1,\n",
        "    rnn_units_2=RNN_UNITS_2,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    trainable_embeddings=True,  # Fine-tune embeddings\n",
        "    name=\"RNN_Skipgram\"\n",
        ")\n",
        "\n",
        "print(\"RNN MODEL: Word2Vec Skip-gram Embedding\")\n",
        "rnn_skipgram.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 RNN with Word2Vec CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build CBOW model\n",
        "rnn_cbow = build_rnn_word2vec(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim_cbow,\n",
        "    embedding_matrix=embedding_matrix_cbow,\n",
        "    rnn_units_1=RNN_UNITS_1,\n",
        "    rnn_units_2=RNN_UNITS_2,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    trainable_embeddings=True,  # Fine-tune embeddings\n",
        "    name=\"RNN_CBOW\"\n",
        ")\n",
        "\n",
        "print(\"RNN MODEL: Word2Vec CBOW Embedding\")\n",
        "rnn_cbow.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Configuration\n",
        "\n",
        "Set up callbacks and class weights for handling imbalanced data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate class weights for imbalanced data\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=EARLY_STOPPING_PATIENCE,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=REDUCE_LR_PATIENCE,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Create directory for saving models\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "model_checkpoint_tfidf = ModelCheckpoint(\n",
        "    'models/rnn_tfidf_best.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_checkpoint_skipgram = ModelCheckpoint(\n",
        "    'models/rnn_skipgram_best.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_checkpoint_cbow = ModelCheckpoint(\n",
        "    'models/rnn_cbow_best.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"‚úì Callbacks configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training and Evaluation\n",
        "\n",
        "### 7.1 Train RNN with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TRAINING: RNN with TF-IDF Embedding\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "history_tfidf = rnn_tfidf.fit(\n",
        "    X_tfidf_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_tfidf_val, y_val),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint_tfidf],\n",
        "    class_weight=class_weight_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "training_time_tfidf = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úì Training completed in {training_time_tfidf:.2f} seconds\")\n",
        "print(f\"‚úì Best validation accuracy: {max(history_tfidf.history['val_accuracy']):.4f}\")\n",
        "print(f\"‚úì Final validation loss: {min(history_tfidf.history['val_loss']):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating RNN with TF-IDF on test set...\")\n",
        "y_pred_tfidf_proba = rnn_tfidf.predict(X_tfidf_test, verbose=0)\n",
        "y_pred_tfidf = (y_pred_tfidf_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "acc_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
        "prec_tfidf = precision_score(y_test, y_pred_tfidf)\n",
        "rec_tfidf = recall_score(y_test, y_pred_tfidf)\n",
        "f1_tfidf = f1_score(y_test, y_pred_tfidf)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST SET RESULTS: RNN with TF-IDF\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy:  {acc_tfidf:.4f}\")\n",
        "print(f\"Precision: {prec_tfidf:.4f}\")\n",
        "print(f\"Recall:    {rec_tfidf:.4f}\")\n",
        "print(f\"F1-Score:  {f1_tfidf:.4f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_tfidf, target_names=['Negative', 'Positive']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].plot(history_tfidf.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "axes[0].plot(history_tfidf.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "axes[0].set_title('RNN + TF-IDF - Accuracy', fontweight='bold', fontsize=12)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "axes[1].plot(history_tfidf.history['loss'], label='Train Loss', linewidth=2)\n",
        "axes[1].plot(history_tfidf.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "axes[1].set_title('RNN + TF-IDF - Loss', fontweight='bold', fontsize=12)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/rnn_tfidf_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix\n",
        "cm_tfidf = confusion_matrix(y_test, y_pred_tfidf)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_tfidf, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.title('RNN + TF-IDF - Confusion Matrix', fontweight='bold', fontsize=12)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/rnn_tfidf_cm.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Train RNN with Word2Vec Skip-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TRAINING: RNN with Word2Vec Skip-gram Embedding\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "history_skipgram = rnn_skipgram.fit(\n",
        "    X_seq_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_seq_val, y_val),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint_skipgram],\n",
        "    class_weight=class_weight_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "training_time_skipgram = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úì Training completed in {training_time_skipgram:.2f} seconds\")\n",
        "print(f\"‚úì Best validation accuracy: {max(history_skipgram.history['val_accuracy']):.4f}\")\n",
        "print(f\"‚úì Final validation loss: {min(history_skipgram.history['val_loss']):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating RNN with Skip-gram on test set...\")\n",
        "y_pred_skipgram_proba = rnn_skipgram.predict(X_seq_test, verbose=0)\n",
        "y_pred_skipgram = (y_pred_skipgram_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "acc_skipgram = accuracy_score(y_test, y_pred_skipgram)\n",
        "prec_skipgram = precision_score(y_test, y_pred_skipgram)\n",
        "rec_skipgram = recall_score(y_test, y_pred_skipgram)\n",
        "f1_skipgram = f1_score(y_test, y_pred_skipgram)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST SET RESULTS: RNN with Word2Vec Skip-gram\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy:  {acc_skipgram:.4f}\")\n",
        "print(f\"Precision: {prec_skipgram:.4f}\")\n",
        "print(f\"Recall:    {rec_skipgram:.4f}\")\n",
        "print(f\"F1-Score:  {f1_skipgram:.4f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_skipgram, target_names=['Negative', 'Positive']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].plot(history_skipgram.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "axes[0].plot(history_skipgram.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "axes[0].set_title('RNN + Skip-gram - Accuracy', fontweight='bold', fontsize=12)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "axes[1].plot(history_skipgram.history['loss'], label='Train Loss', linewidth=2)\n",
        "axes[1].plot(history_skipgram.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "axes[1].set_title('RNN + Skip-gram - Loss', fontweight='bold', fontsize=12)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/rnn_skipgram_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix\n",
        "cm_skipgram = confusion_matrix(y_test, y_pred_skipgram)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_skipgram, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.title('RNN + Skip-gram - Confusion Matrix', fontweight='bold', fontsize=12)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/rnn_skipgram_cm.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Train RNN with Word2Vec CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TRAINING: RNN with Word2Vec CBOW Embedding\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "history_cbow = rnn_cbow.fit(\n",
        "    X_seq_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_seq_val, y_val),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint_cbow],\n",
        "    class_weight=class_weight_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "training_time_cbow = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úì Training completed in {training_time_cbow:.2f} seconds\")\n",
        "print(f\"‚úì Best validation accuracy: {max(history_cbow.history['val_accuracy']):.4f}\")\n",
        "print(f\"‚úì Final validation loss: {min(history_cbow.history['val_loss']):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating RNN with CBOW on test set...\")\n",
        "y_pred_cbow_proba = rnn_cbow.predict(X_seq_test, verbose=0)\n",
        "y_pred_cbow = (y_pred_cbow_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "acc_cbow = accuracy_score(y_test, y_pred_cbow)\n",
        "prec_cbow = precision_score(y_test, y_pred_cbow)\n",
        "rec_cbow = recall_score(y_test, y_pred_cbow)\n",
        "f1_cbow = f1_score(y_test, y_pred_cbow)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST SET RESULTS: RNN with Word2Vec CBOW\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy:  {acc_cbow:.4f}\")\n",
        "print(f\"Precision: {prec_cbow:.4f}\")\n",
        "print(f\"Recall:    {rec_cbow:.4f}\")\n",
        "print(f\"F1-Score:  {f1_cbow:.4f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_cbow, target_names=['Negative', 'Positive']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].plot(history_cbow.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "axes[0].plot(history_cbow.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "axes[0].set_title('RNN + CBOW - Accuracy', fontweight='bold', fontsize=12)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "axes[1].plot(history_cbow.history['loss'], label='Train Loss', linewidth=2)\n",
        "axes[1].plot(history_cbow.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "axes[1].set_title('RNN + CBOW - Loss', fontweight='bold', fontsize=12)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/rnn_cbow_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix\n",
        "cm_cbow = confusion_matrix(y_test, y_pred_cbow)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_cbow, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.title('RNN + CBOW - Confusion Matrix', fontweight='bold', fontsize=12)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/rnn_cbow_cm.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comparative Analysis\n",
        "\n",
        "Compare performance across all three embedding approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "comparison_results = pd.DataFrame({\n",
        "    'Embedding': ['TF-IDF', 'Word2Vec Skip-gram', 'Word2Vec CBOW'],\n",
        "    'Accuracy': [acc_tfidf, acc_skipgram, acc_cbow],\n",
        "    'Precision': [prec_tfidf, prec_skipgram, prec_cbow],\n",
        "    'Recall': [rec_tfidf, rec_skipgram, rec_cbow],\n",
        "    'F1-Score': [f1_tfidf, f1_skipgram, f1_cbow],\n",
        "    'Training Time (s)': [training_time_tfidf, training_time_skipgram, training_time_cbow],\n",
        "    'Best Val Accuracy': [\n",
        "        max(history_tfidf.history['val_accuracy']),\n",
        "        max(history_skipgram.history['val_accuracy']),\n",
        "        max(history_cbow.history['val_accuracy'])\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RNN MODELS PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_results.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Identify best model\n",
        "best_idx = comparison_results['Accuracy'].idxmax()\n",
        "best_model = comparison_results.loc[best_idx, 'Embedding']\n",
        "print(f\"\\nüèÜ BEST MODEL: {best_model}\")\n",
        "print(f\"   Accuracy: {comparison_results.loc[best_idx, 'Accuracy']:.4f}\")\n",
        "print(f\"   F1-Score: {comparison_results.loc[best_idx, 'F1-Score']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "models = comparison_results['Embedding']\n",
        "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
        "\n",
        "# Accuracy comparison\n",
        "axes[0, 0].bar(models, comparison_results['Accuracy'], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[0, 0].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[0, 0].set_title('Accuracy Comparison', fontweight='bold', fontsize=14)\n",
        "axes[0, 0].set_ylim([0, 1])\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(comparison_results['Accuracy']):\n",
        "    axes[0, 0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold', fontsize=11)\n",
        "axes[0, 0].tick_params(axis='x', rotation=15)\n",
        "\n",
        "# Precision comparison\n",
        "axes[0, 1].bar(models, comparison_results['Precision'], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[0, 1].set_ylabel('Precision', fontsize=12)\n",
        "axes[0, 1].set_title('Precision Comparison', fontweight='bold', fontsize=14)\n",
        "axes[0, 1].set_ylim([0, 1])\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(comparison_results['Precision']):\n",
        "    axes[0, 1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold', fontsize=11)\n",
        "axes[0, 1].tick_params(axis='x', rotation=15)\n",
        "\n",
        "# Recall comparison\n",
        "axes[1, 0].bar(models, comparison_results['Recall'], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[1, 0].set_ylabel('Recall', fontsize=12)\n",
        "axes[1, 0].set_title('Recall Comparison', fontweight='bold', fontsize=14)\n",
        "axes[1, 0].set_ylim([0, 1])\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(comparison_results['Recall']):\n",
        "    axes[1, 0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold', fontsize=11)\n",
        "axes[1, 0].tick_params(axis='x', rotation=15)\n",
        "\n",
        "# F1-Score comparison\n",
        "axes[1, 1].bar(models, comparison_results['F1-Score'], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[1, 1].set_ylabel('F1-Score', fontsize=12)\n",
        "axes[1, 1].set_title('F1-Score Comparison', fontweight='bold', fontsize=14)\n",
        "axes[1, 1].set_ylim([0, 1])\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(comparison_results['F1-Score']):\n",
        "    axes[1, 1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold', fontsize=11)\n",
        "axes[1, 1].tick_params(axis='x', rotation=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/rnn_comparison_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Side-by-side training curves comparison\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# TF-IDF curves\n",
        "axes[0, 0].plot(history_tfidf.history['accuracy'], label='Train', linewidth=2)\n",
        "axes[0, 0].plot(history_tfidf.history['val_accuracy'], label='Val', linewidth=2)\n",
        "axes[0, 0].set_title('TF-IDF - Accuracy', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].plot(history_tfidf.history['loss'], label='Train', linewidth=2)\n",
        "axes[1, 0].plot(history_tfidf.history['val_loss'], label='Val', linewidth=2)\n",
        "axes[1, 0].set_title('TF-IDF - Loss', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Loss')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Skip-gram curves\n",
        "axes[0, 1].plot(history_skipgram.history['accuracy'], label='Train', linewidth=2)\n",
        "axes[0, 1].plot(history_skipgram.history['val_accuracy'], label='Val', linewidth=2)\n",
        "axes[0, 1].set_title('Skip-gram - Accuracy', fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].plot(history_skipgram.history['loss'], label='Train', linewidth=2)\n",
        "axes[1, 1].plot(history_skipgram.history['val_loss'], label='Val', linewidth=2)\n",
        "axes[1, 1].set_title('Skip-gram - Loss', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Loss')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# CBOW curves\n",
        "axes[0, 2].plot(history_cbow.history['accuracy'], label='Train', linewidth=2)\n",
        "axes[0, 2].plot(history_cbow.history['val_accuracy'], label='Val', linewidth=2)\n",
        "axes[0, 2].set_title('CBOW - Accuracy', fontweight='bold')\n",
        "axes[0, 2].set_xlabel('Epoch')\n",
        "axes[0, 2].set_ylabel('Accuracy')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 2].plot(history_cbow.history['loss'], label='Train', linewidth=2)\n",
        "axes[1, 2].plot(history_cbow.history['val_loss'], label='Val', linewidth=2)\n",
        "axes[1, 2].set_title('CBOW - Loss', fontweight='bold')\n",
        "axes[1, 2].set_xlabel('Epoch')\n",
        "axes[1, 2].set_ylabel('Loss')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/rnn_training_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrices comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# TF-IDF confusion matrix\n",
        "sns.heatmap(cm_tfidf, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "axes[0].set_title('TF-IDF', fontweight='bold', fontsize=12)\n",
        "axes[0].set_ylabel('True Label')\n",
        "axes[0].set_xlabel('Predicted Label')\n",
        "\n",
        "# Skip-gram confusion matrix\n",
        "sns.heatmap(cm_skipgram, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "axes[1].set_title('Skip-gram', fontweight='bold', fontsize=12)\n",
        "axes[1].set_ylabel('True Label')\n",
        "axes[1].set_xlabel('Predicted Label')\n",
        "\n",
        "# CBOW confusion matrix\n",
        "sns.heatmap(cm_cbow, annot=True, fmt='d', cmap='Blues', ax=axes[2],\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "axes[2].set_title('CBOW', fontweight='bold', fontsize=12)\n",
        "axes[2].set_ylabel('True Label')\n",
        "axes[2].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.suptitle('Confusion Matrices Comparison', fontweight='bold', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/rnn_confusion_matrices_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Results Summary and Discussion\n",
        "\n",
        "### Key Findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion\n",
        "\n",
        "1. **Best Performing Embedding**: Based on the test set results, [best model] achieved the highest accuracy of [X]%.\n",
        "\n",
        "2. **TF-IDF Performance**: TF-IDF provides a statistical baseline without semantic understanding. It captures term importance but lacks the contextual relationships that neural embeddings provide.\n",
        "\n",
        "3. **Skip-gram vs CBOW**: \n",
        "   - Skip-gram typically performs better on rare words and captures more semantic relationships\n",
        "   - CBOW is faster to train and often works better for frequent words\n",
        "   - The performance difference reflects their different learning objectives\n",
        "\n",
        "4. **RNN Architecture**: The SimpleRNN architecture with two layers (64 ‚Üí 32 units) effectively captured sequential patterns in the drug review text. The dropout regularization helped prevent overfitting.\n",
        "\n",
        "5. **Training Efficiency**: Word2Vec embeddings required pre-training time, but the RNN models with these embeddings generally converged faster than TF-IDF.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- SimpleRNN may struggle with long-term dependencies compared to LSTM or GRU\n",
        "- The fixed sequence length (100 tokens) may truncate important information in longer reviews\n",
        "- Class imbalance was addressed with class weights, but further techniques like SMOTE could be explored\n",
        "- Hyperparameters were not extensively tuned; grid search could improve performance\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "- For production use, consider the best-performing embedding identified above\n",
        "- Experiment with bidirectional RNNs or LSTM/GRU architectures for better long-term dependency modeling\n",
        "- Fine-tune hyperparameters systematically using validation set performance\n",
        "- Consider ensemble methods combining multiple embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Hyperparameter Documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Document all hyperparameters used\n",
        "hyperparams_doc = {\n",
        "    'Model Architecture': {\n",
        "        'Type': 'SimpleRNN',\n",
        "        'RNN Layer 1 Units': RNN_UNITS_1,\n",
        "        'RNN Layer 2 Units': RNN_UNITS_2,\n",
        "        'Dense Layer Units': 32,\n",
        "        'Output Activation': 'sigmoid',\n",
        "        'Dropout Rate': DROPOUT_RATE\n",
        "    },\n",
        "    'Training Configuration': {\n",
        "        'Batch Size': BATCH_SIZE,\n",
        "        'Max Epochs': EPOCHS,\n",
        "        'Learning Rate': LEARNING_RATE,\n",
        "        'Optimizer': 'Adam',\n",
        "        'Loss Function': 'binary_crossentropy',\n",
        "        'Early Stopping Patience': EARLY_STOPPING_PATIENCE,\n",
        "        'Reduce LR Patience': REDUCE_LR_PATIENCE\n",
        "    },\n",
        "    'Embedding Configurations': {\n",
        "        'TF-IDF Features': EMBEDDING_DIMS['tfidf'],\n",
        "        'Word2Vec Skip-gram Dim': EMBEDDING_DIMS['word2vec_skipgram'],\n",
        "        'Word2Vec CBOW Dim': EMBEDDING_DIMS['word2vec_cbow'],\n",
        "        'Word2Vec Window Size': W2V_WINDOW_SIZE,\n",
        "        'Word2Vec Min Count': W2V_MIN_COUNT,\n",
        "        'Word2Vec Training Epochs': W2V_EPOCHS\n",
        "    },\n",
        "    'Data Configuration': {\n",
        "        'Sequence Length': X_seq_train.shape[1],\n",
        "        'Vocabulary Size': vocab_size,\n",
        "        'Random Seed': RANDOM_STATE,\n",
        "        'Class Weights': class_weight_dict\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"HYPERPARAMETER DOCUMENTATION\")\n",
        "print(\"=\"*80)\n",
        "for category, params in hyperparams_doc.items():\n",
        "    print(f\"\\n{category}:\")\n",
        "    for key, value in params.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save results to CSV for team comparison\n",
        "comparison_results.to_csv('models/rnn_results_summary.csv', index=False)\n",
        "print(\"\\n‚úì Results saved to models/rnn_results_summary.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. References\n",
        "\n",
        "- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. *arXiv preprint arXiv:1301.3781*.\n",
        "\n",
        "- Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. *Advances in neural information processing systems*, 26.\n",
        "\n",
        "- Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors for word representation. *Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)*, 1532-1543.\n",
        "\n",
        "- Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *Neural computation*, 9(8), 1735-1780.\n",
        "\n",
        "- Cho, K., Van Merri√´nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. *arXiv preprint arXiv:1406.1078*."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
