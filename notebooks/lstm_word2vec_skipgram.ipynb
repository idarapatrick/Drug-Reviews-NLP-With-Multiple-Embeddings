{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864ff7fc",
   "metadata": {},
   "source": [
    "# LSTM Model with Word2Vec Skip-gram Embeddings\n",
    "## Drug Reviews Classification - Gershom\n",
    "\n",
    "**Model**: LSTM (Long Short-Term Memory) / BiLSTM\n",
    "**Embedding**: Word2Vec Skip-gram (200-dim, medium config)\n",
    "**Task**: Drug review rating prediction\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Structure:\n",
    "1. Setup and Data Loading\n",
    "2. Word2Vec Embedding Training\n",
    "3. LSTM Model Architecture\n",
    "4. Training & Evaluation\n",
    "5. Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be307e3a",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e2afd2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      3\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m../\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Bidirectional, Dense, Dropout, Embedding, \n",
    "    Input, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Shared modules\n",
    "from src.data_utils import DataLoader, create_dataset_from_dataframe\n",
    "from src.preprocessing import get_preprocessor, TextPreprocessor\n",
    "from src.eda import EDAAnalyzer\n",
    "from embeddings.word2vec_embedding import Word2VecEmbedding, get_word2vec_embedding\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b95159",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a23700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TRAIN_PATH = '../data/drug_review_train.csv'\n",
    "VAL_PATH = '../data/drug_review_validation.csv'\n",
    "TEST_PATH = '../data/drug_review_test.csv'\n",
    "\n",
    "# Data columns\n",
    "TEXT_COLUMN = 'review'\n",
    "LABEL_COLUMN = 'rating'\n",
    "\n",
    "# Preprocessing\n",
    "PREPROCESSING_CONFIG = 'moderate'  # minimal, moderate, or aggressive\n",
    "\n",
    "# Embedding configuration\n",
    "EMBEDDING_TYPE = 'word2vec'\n",
    "EMBEDDING_CONFIG = 'skipgram_medium'  # 200-dim, 10 epochs\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "# Model architecture (OPTIMIZED FOR SPEED)\n",
    "USE_BIDIRECTIONAL = True  # Set to True for BiLSTM, False for LSTM\n",
    "LSTM_UNITS = 64  # Reduced from 128 → 64 for faster training\n",
    "DROPOUT_RATE = 0.3\n",
    "RECURRENT_DROPOUT = 0.2\n",
    "DENSE_UNITS = 64\n",
    "\n",
    "# Training (OPTIMIZED FOR SPEED)\n",
    "BATCH_SIZE = 64  # Increased from 32 → 64 for faster training\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "PATIENCE = 3\n",
    "\n",
    "# Sequence parameters (OPTIMIZED FOR SPEED)\n",
    "MAX_SEQUENCE_LENGTH = 150  # Reduced from 200 → 150 for faster processing\n",
    "VOCAB_SIZE = 10000  # Maximum vocabulary size\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Configuration set! (OPTIMIZED FOR SPEED)\")\n",
    "print(f\"Model type: {'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}\")\n",
    "print(f\"Embedding: {EMBEDDING_TYPE} ({EMBEDDING_CONFIG})\")\n",
    "print(f\"LSTM units: {LSTM_UNITS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} (larger = faster)\")\n",
    "print(f\"Max sequence length: {MAX_SEQUENCE_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c44790",
   "metadata": {},
   "source": [
    "### Performance Tips:\n",
    "**For faster training:**\n",
    "- ✅ Batch size increased: 32 → 64 (processes more samples per step)\n",
    "- ✅ LSTM units reduced: 128 → 64 (smaller model = faster)\n",
    "- ✅ Sequence length reduced: 200 → 150 (less computation per sample)\n",
    "\n",
    "**Expected speedup: 2-3x faster per epoch**\n",
    "\n",
    "**If still slow:**\n",
    "- Use Google Colab with GPU (free T4 GPU = 5-10x faster)\n",
    "- Set `USE_BIDIRECTIONAL = False` (LSTM is 2x faster than BiLSTM)\n",
    "- Reduce to 1 LSTM layer in model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba64664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability for faster training\n",
    "print(\"=\"*50)\n",
    "print(\"HARDWARE CHECK\")\n",
    "print(\"=\"*50)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"✓ GPU AVAILABLE: {len(gpus)} GPU(s) detected\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu.name}\")\n",
    "    print(\"\\nExpected training time: ~5-10 minutes per epoch\")\n",
    "else:\n",
    "    print(\"⚠ NO GPU DETECTED - Running on CPU\")\n",
    "    print(\"Expected training time: ~20-30 minutes per epoch\")\n",
    "    print(\"\\nTo speed up:\")\n",
    "    print(\"1. Use Google Colab (free GPU)\")\n",
    "    print(\"2. Or reduce model complexity further (see options below)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc5746",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa44194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "val_df = pd.read_csv(VAL_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(val_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "print(f\"\\nClass distribution (train):\")\n",
    "print(train_df[LABEL_COLUMN].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fecead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "import nltk\n",
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)  # Open Multilingual WordNet\n",
    "print(\"✓ NLTK data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = get_preprocessor(PREPROCESSING_CONFIG)\n",
    "print(f\"Using '{PREPROCESSING_CONFIG}' preprocessing configuration\")\n",
    "\n",
    "# Get tokenized texts (Word2Vec needs tokens, not strings)\n",
    "print(\"\\nPreprocessing and tokenizing...\")\n",
    "train_texts = train_df[TEXT_COLUMN].fillna('').tolist()\n",
    "val_texts = val_df[TEXT_COLUMN].fillna('').tolist()\n",
    "test_texts = test_df[TEXT_COLUMN].fillna('').tolist()\n",
    "\n",
    "# Get tokenized versions for Word2Vec training\n",
    "train_tokens = preprocessor.get_tokens_batch(train_texts)\n",
    "val_tokens = preprocessor.get_tokens_batch(val_texts)\n",
    "test_tokens = preprocessor.get_tokens_batch(test_texts)\n",
    "\n",
    "# Extract labels\n",
    "train_labels = train_df[LABEL_COLUMN].values\n",
    "val_labels = val_df[LABEL_COLUMN].values\n",
    "test_labels = test_df[LABEL_COLUMN].values\n",
    "\n",
    "print(f\"✓ Tokenization complete!\")\n",
    "print(f\"Example tokenized review: {train_tokens[0][:20]}...\")  # First 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c9bfca",
   "metadata": {},
   "source": [
    "## 4. Train Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12109f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Word2Vec\n",
    "print(\"Training Word2Vec Skip-gram embeddings...\")\n",
    "w2v_model = get_word2vec_embedding(EMBEDDING_CONFIG)\n",
    "w2v_model.fit(train_tokens)\n",
    "\n",
    "print(\"\\n✓ Word2Vec training complete!\")\n",
    "print(f\"Model info: {w2v_model.get_model_info()}\")\n",
    "print(f\"Vocabulary size: {w2v_model.get_vocabulary_size()}\")\n",
    "print(f\"Embedding dimension: {w2v_model.embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Word2Vec embeddings with sample words\n",
    "print(\"\\nTesting Word2Vec embeddings...\")\n",
    "test_words = ['pain', 'drug', 'effective', 'side', 'effect']\n",
    "\n",
    "for word in test_words:\n",
    "    try:\n",
    "        similar = w2v_model.most_similar(word, topn=5)\n",
    "        print(f\"\\n'{word}' most similar:\")\n",
    "        for sim_word, score in similar:\n",
    "            print(f\"  {sim_word}: {score:.3f}\")\n",
    "    except KeyError:\n",
    "        print(f\"\\n'{word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4045fa6",
   "metadata": {},
   "source": [
    "## 5. Create Sequences and Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from Word2Vec\n",
    "vocab = w2v_model.get_vocab()\n",
    "word_to_idx = {word: idx + 1 for idx, word in enumerate(vocab[:VOCAB_SIZE-1])}  # Reserve 0 for padding\n",
    "word_to_idx['<PAD>'] = 0\n",
    "word_to_idx['<UNK>'] = len(word_to_idx)\n",
    "\n",
    "print(f\"Vocabulary size (with special tokens): {len(word_to_idx)}\")\n",
    "\n",
    "# Convert tokens to sequences\n",
    "def tokens_to_sequences(token_lists, word_to_idx):\n",
    "    sequences = []\n",
    "    for tokens in token_lists:\n",
    "        seq = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "train_sequences = tokens_to_sequences(train_tokens, word_to_idx)\n",
    "val_sequences = tokens_to_sequences(val_tokens, word_to_idx)\n",
    "test_sequences = tokens_to_sequences(test_tokens, word_to_idx)\n",
    "\n",
    "# Pad sequences\n",
    "train_padded = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "val_padded = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "print(f\"\\nSequence shapes:\")\n",
    "print(f\"Train: {train_padded.shape}\")\n",
    "print(f\"Val: {val_padded.shape}\")\n",
    "print(f\"Test: {test_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b14372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix from Word2Vec\n",
    "print(\"\\nCreating embedding matrix...\")\n",
    "embedding_matrix = np.zeros((len(word_to_idx), EMBEDDING_DIM))\n",
    "\n",
    "for word, idx in word_to_idx.items():\n",
    "    if word not in ['<PAD>', '<UNK>']:\n",
    "        try:\n",
    "            embedding_matrix[idx] = w2v_model.get_word_vector(word)\n",
    "        except KeyError:\n",
    "            # Initialize with small random values\n",
    "            embedding_matrix[idx] = np.random.randn(EMBEDDING_DIM) * 0.01\n",
    "\n",
    "print(f\"✓ Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "print(f\"Non-zero rows: {np.count_nonzero(embedding_matrix.any(axis=1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebfefdb",
   "metadata": {},
   "source": [
    "## 6. Process Labels for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba866bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the rating distribution to decide on classification strategy\n",
    "print(\"Rating distribution:\")\n",
    "print(train_df[LABEL_COLUMN].value_counts().sort_index())\n",
    "\n",
    "# Convert ratings to classes (adjust based on your dataset)\n",
    "# Option 1: Binary classification (positive vs negative)\n",
    "# Option 2: Multi-class classification (ratings 1-10)\n",
    "# Option 3: Regression (predict exact rating)\n",
    "\n",
    "# For this example, let's use binary classification: ratings >= 6 = positive (1), < 6 = negative (0)\n",
    "CLASSIFICATION_TYPE = 'binary'  # 'binary', 'multiclass', or 'regression'\n",
    "THRESHOLD = 6  # For binary classification\n",
    "\n",
    "if CLASSIFICATION_TYPE == 'binary':\n",
    "    train_y = (train_labels >= THRESHOLD).astype(int)\n",
    "    val_y = (val_labels >= THRESHOLD).astype(int)\n",
    "    test_y = (test_labels >= THRESHOLD).astype(int)\n",
    "    NUM_CLASSES = 2\n",
    "    print(f\"\\nBinary classification: >= {THRESHOLD} = positive\")\n",
    "    print(f\"Train class distribution: {np.bincount(train_y)}\")\n",
    "elif CLASSIFICATION_TYPE == 'multiclass':\n",
    "    train_y = train_labels.astype(int) - 1  # Assuming ratings 1-10, convert to 0-9\n",
    "    val_y = val_labels.astype(int) - 1\n",
    "    test_y = test_labels.astype(int) - 1\n",
    "    NUM_CLASSES = len(np.unique(train_y))\n",
    "    print(f\"\\nMulti-class classification: {NUM_CLASSES} classes\")\n",
    "else:  # regression\n",
    "    train_y = train_labels.astype(float)\n",
    "    val_y = val_labels.astype(float)\n",
    "    test_y = test_labels.astype(float)\n",
    "    NUM_CLASSES = 1\n",
    "    print(\"\\nRegression: Predicting exact rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c00707",
   "metadata": {},
   "source": [
    "## 7. Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    embedding_matrix,\n",
    "    max_length,\n",
    "    lstm_units=128,\n",
    "    dropout_rate=0.3,\n",
    "    recurrent_dropout=0.2,\n",
    "    dense_units=64,\n",
    "    num_classes=2,\n",
    "    use_bidirectional=True,\n",
    "    classification_type='binary'\n",
    "):\n",
    "    \"\"\"\n",
    "    Build LSTM or BiLSTM model for text classification.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        embedding_dim: Dimension of embeddings\n",
    "        embedding_matrix: Pre-trained embedding matrix\n",
    "        max_length: Maximum sequence length\n",
    "        lstm_units: Number of LSTM units\n",
    "        dropout_rate: Dropout rate\n",
    "        recurrent_dropout: Recurrent dropout rate\n",
    "        dense_units: Dense layer units\n",
    "        num_classes: Number of output classes\n",
    "        use_bidirectional: Use BiLSTM if True\n",
    "        classification_type: 'binary', 'multiclass', or 'regression'\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer with pre-trained Word2Vec weights\n",
    "    model.add(Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_length,\n",
    "        trainable=False,  # Freeze Word2Vec embeddings (can set to True for fine-tuning)\n",
    "        name='embedding'\n",
    "    ))\n",
    "    \n",
    "    # LSTM layer(s)\n",
    "    if use_bidirectional:\n",
    "        model.add(Bidirectional(\n",
    "            LSTM(\n",
    "                lstm_units,\n",
    "                return_sequences=True,\n",
    "                dropout=dropout_rate,\n",
    "                recurrent_dropout=recurrent_dropout\n",
    "            ),\n",
    "            name='bilstm_1'\n",
    "        ))\n",
    "        # Second BiLSTM layer\n",
    "        model.add(Bidirectional(\n",
    "            LSTM(\n",
    "                lstm_units // 2,\n",
    "                dropout=dropout_rate,\n",
    "                recurrent_dropout=recurrent_dropout\n",
    "            ),\n",
    "            name='bilstm_2'\n",
    "        ))\n",
    "    else:\n",
    "        model.add(LSTM(\n",
    "            lstm_units,\n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            name='lstm_1'\n",
    "        ))\n",
    "        model.add(LSTM(\n",
    "            lstm_units // 2,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            name='lstm_2'\n",
    "        ))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(dense_units, activation='relu', name='dense_1'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    if classification_type == 'binary':\n",
    "        model.add(Dense(1, activation='sigmoid', name='output'))\n",
    "        loss = 'binary_crossentropy'\n",
    "        metrics = ['accuracy']\n",
    "    elif classification_type == 'multiclass':\n",
    "        model.add(Dense(num_classes, activation='softmax', name='output'))\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "        metrics = ['accuracy']\n",
    "    else:  # regression\n",
    "        model.add(Dense(1, activation='linear', name='output'))\n",
    "        loss = 'mse'\n",
    "        metrics = ['mae']\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "print(\"Building LSTM model...\")\n",
    "model = build_lstm_model(\n",
    "    vocab_size=len(word_to_idx),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    recurrent_dropout=RECURRENT_DROPOUT,\n",
    "    dense_units=DENSE_UNITS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    use_bidirectional=USE_BIDIRECTIONAL,\n",
    "    classification_type=CLASSIFICATION_TYPE\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Model built successfully!\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0f2e6",
   "metadata": {},
   "source": [
    "## 8. Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff678cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='best_lstm_word2vec_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"- Early stopping (patience=3)\")\n",
    "print(\"- Learning rate reduction\")\n",
    "print(\"- Model checkpointing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c46ac8",
   "metadata": {},
   "source": [
    "## 9. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f76255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "print(f\"Model: {'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max epochs: {EPOCHS}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "history = model.fit(\n",
    "    train_padded,\n",
    "    train_y,\n",
    "    validation_data=(val_padded, val_y),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8eab2",
   "metadata": {},
   "source": [
    "## 10. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy plot (for classification)\n",
    "if CLASSIFICATION_TYPE in ['binary', 'multiclass']:\n",
    "    axes[1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "else:  # regression\n",
    "    axes[1].plot(history.history['mae'], label='Train MAE', linewidth=2)\n",
    "    axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "    axes[1].set_title('Model MAE', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2ebe1",
   "metadata": {},
   "source": [
    "## 11. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3725bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_results = model.evaluate(test_padded, test_y, verbose=1)\n",
    "\n",
    "if CLASSIFICATION_TYPE in ['binary', 'multiclass']:\n",
    "    print(f\"\\nTest Loss: {test_results[0]:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_results[1]:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nTest Loss (MSE): {test_results[0]:.4f}\")\n",
    "    print(f\"Test MAE: {test_results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "test_predictions = model.predict(test_padded, verbose=1)\n",
    "\n",
    "if CLASSIFICATION_TYPE == 'binary':\n",
    "    test_pred_classes = (test_predictions > 0.5).astype(int).flatten()\n",
    "elif CLASSIFICATION_TYPE == 'multiclass':\n",
    "    test_pred_classes = np.argmax(test_predictions, axis=1)\n",
    "else:  # regression\n",
    "    test_pred_classes = test_predictions.flatten()\n",
    "\n",
    "print(\"✓ Predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5a118",
   "metadata": {},
   "source": [
    "## 12. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d867ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLASSIFICATION_TYPE in ['binary', 'multiclass']:\n",
    "    # Classification report\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(test_y, test_pred_classes, digits=4))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(test_y, test_pred_classes)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(test_y, test_pred_classes)\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Support': support\n",
    "    })\n",
    "    \n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    print(metrics_df.to_string())\n",
    "else:\n",
    "    # Regression metrics\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    mse = mean_squared_error(test_y, test_pred_classes)\n",
    "    mae = mean_absolute_error(test_y, test_pred_classes)\n",
    "    r2 = r2_score(test_y, test_pred_classes)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"REGRESSION METRICS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(test_y, test_pred_classes, alpha=0.5)\n",
    "    plt.plot([test_y.min(), test_y.max()], [test_y.min(), test_y.max()], 'r--', lw=2)\n",
    "    plt.xlabel('True Rating')\n",
    "    plt.ylabel('Predicted Rating')\n",
    "    plt.title('Predicted vs True Ratings')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bbd5bf",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96aa1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results dictionary\n",
    "results = {\n",
    "    'model': 'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM',\n",
    "    'embedding': f'{EMBEDDING_TYPE}_{EMBEDDING_CONFIG}',\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'lstm_units': LSTM_UNITS,\n",
    "    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "    'vocab_size': len(word_to_idx),\n",
    "    'preprocessing': PREPROCESSING_CONFIG,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs_trained': len(history.history['loss']),\n",
    "    'classification_type': CLASSIFICATION_TYPE,\n",
    "}\n",
    "\n",
    "if CLASSIFICATION_TYPE in ['binary', 'multiclass']:\n",
    "    results['test_accuracy'] = float(test_results[1])\n",
    "    results['test_loss'] = float(test_results[0])\n",
    "    \n",
    "    # Add precision, recall, f1\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(test_y, test_pred_classes, average='weighted')\n",
    "    results['precision'] = float(precision)\n",
    "    results['recall'] = float(recall)\n",
    "    results['f1_score'] = float(f1)\n",
    "else:\n",
    "    results['test_mse'] = float(test_results[0])\n",
    "    results['test_mae'] = float(test_results[1])\n",
    "    results['r2_score'] = float(r2_score(test_y, test_pred_classes))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Save results to JSON\n",
    "import json\n",
    "with open('lstm_word2vec_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Results saved to 'lstm_word2vec_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74c243",
   "metadata": {},
   "source": [
    "## 14. Model Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LSTM + WORD2VEC SKIP-GRAM - EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel: {'BiLSTM' if USE_BIDIRECTIONAL else 'LSTM'}\")\n",
    "print(f\"Embedding: Word2Vec Skip-gram (200-dim)\")\n",
    "print(f\"Total Parameters: {model.count_params():,}\")\n",
    "\n",
    "if CLASSIFICATION_TYPE in ['binary', 'multiclass']:\n",
    "    print(f\"\\nFinal Test Accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"F1-Score: {results['f1_score']:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nFinal Test MAE: {results['test_mae']:.4f}\")\n",
    "    print(f\"R² Score: {results['r2_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"1. Create lstm_glove.ipynb for GloVe embeddings\")\n",
    "print(\"2. Create lstm_tfidf.ipynb for TF-IDF embeddings\")\n",
    "print(\"3. Compare performance across all three embeddings\")\n",
    "print(\"4. Document findings for final report\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
