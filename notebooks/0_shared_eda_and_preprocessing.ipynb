{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c75b6c2d",
   "metadata": {},
   "source": [
    "# Shared Data Preprocessing And EDA Module\n",
    "## Drug Reviews NLP - Group Project\n",
    "\n",
    "This is a shared notebook across all team members that shows how to use the shared preprocessing and EDA utilities for the drug reviews dataset. All team members can call these functions to prepare their data consistently before implementing their specific models (GRU, RNN, LSTM, Transformer).\n",
    "\n",
    "**Important**: After running this notebook, you can simply import the functions into your own model notebooks without rewriting the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b6c22",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, we import all necessary libraries for data processing, analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import shared modules from src/\n",
    "from src.data_utils import create_dataset_from_dataframe, DrugReviewDataset\n",
    "from src.preprocessing import TextPreprocessor, get_preprocessor, AdvancedTextPreprocessor\n",
    "from src.eda import EDAAnalyzer\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c87e8b2",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Drug Reviews Dataset\n",
    "\n",
    "Before processing, let's load the data and understand its structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c630fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Dataset: drugLibTrain_raw.tsv - Tab-separated values file\n",
    "DATA_PATH = '../data/drugLibTrain_raw.tsv'\n",
    "TEXT_COLUMN = 'commentsReview'  # Main review text column\n",
    "LABEL_COLUMN = 'rating'  # Target variable (rating score)\n",
    "BENEFITS_COLUMN = 'benefitsReview'  # Optional additional text\n",
    "SIDE_EFFECTS_COLUMN = 'sideEffectsReview'  # Optional additional text\n",
    "DRUG_COLUMN = 'urlDrugName'  # Drug identifier\n",
    "\n",
    "# Load TSV file directly with pandas\n",
    "df = pd.read_csv(DATA_PATH, sep='\\t')\n",
    "\n",
    "print(f\"Dataset loaded successfully from {DATA_PATH}!\")\n",
    "print(f\"File format: Tab-separated values (TSV)\")\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nFirst 3 rows:\")df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d9b41b",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Use the EDAAnalyzer to understand the dataset before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a5eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EDA Analyzer\n",
    "analyzer = EDAAnalyzer(df)\n",
    "analyzer.set_columns(text_column=TEXT_COLUMN, label_column=LABEL_COLUMN)\n",
    "\n",
    "# Generate comprehensive report\n",
    "print(analyzer.generate_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Class distribution bar plot\n",
    "analyzer.plot_label_distribution()\n",
    "\n",
    "# Text length distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "texts = df[TEXT_COLUMN].dropna()\n",
    "text_lengths = texts.str.split().str.len()\n",
    "plt.hist(text_lengths, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "plt.title('Distribution of Review Lengths (Word Count)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average review length: {text_lengths.mean():.1f} words\")\n",
    "print(f\"Median review length: {text_lengths.median():.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48defba3",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing\n",
    "\n",
    "Different models might prefer different preprocessing levels. Our team has chosen:\n",
    "- **minimal**: For Word2Vec embeddings (preserves raw semantic information)\n",
    "- **moderate**: General-purpose (RECOMMENDED for most cases)\n",
    "- **aggressive**: For TF-IDF embeddings (maximizes statistical features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e690017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_CONFIG = 'moderate'  # balances cleaning without over-preprocessing\n",
    "\n",
    "preprocessor = get_preprocessor(PREPROCESSING_CONFIG)\n",
    "\n",
    "# Function to safely handle None/NaN values during preprocessing\n",
    "def safe_preprocess(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return preprocessor.process(str(text))\n",
    "\n",
    "# Apply preprocessing to all texts\n",
    "print(\"\\nPreprocessing all texts... (this may take a moment)\")\n",
    "processed_texts = df[TEXT_COLUMN].apply(safe_preprocess)\n",
    "\n",
    "# Show before/after example\n",
    "print(\"\\nExample - Before and After:\")\n",
    "sample_idx = 0\n",
    "print(f\"\\nOriginal: {df[TEXT_COLUMN].iloc[sample_idx][:100]}\")\n",
    "print(f\"\\nProcessed: {processed_texts.iloc[sample_idx][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8fe527",
   "metadata": {},
   "source": [
    "## 5. Create Dataset Object and Split Data\n",
    "\n",
    "Create a DrugReviewDataset object and split into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "dataset = create_dataset_from_dataframe(\n",
    "    df=df,\n",
    "    text_column=TEXT_COLUMN,\n",
    "    label_column=LABEL_COLUMN,\n",
    "    drug_column=DRUG_COLUMN if DRUG_COLUMN in df.columns else None\n",
    ")\n",
    "\n",
    "# Set processed texts\n",
    "dataset.set_processed_texts(processed_texts.tolist())\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "print(f\"Class distribution: {dataset.class_distribution()}\")\n",
    "\n",
    "# Split data using sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split to separate test set with 20 percent of the data\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[LABEL_COLUMN],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split to separate validation set from training with 10 percent\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.1 / 0.8,  # Adjust ratio for the remaining data\n",
    "    stratify=train_val_df[LABEL_COLUMN],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Val set: {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Create separate dataset objects for each split\n",
    "train_dataset = create_dataset_from_dataframe(\n",
    "    df=train_df,\n",
    "    text_column=TEXT_COLUMN,\n",
    "    label_column=LABEL_COLUMN,\n",
    "    drug_column=DRUG_COLUMN if DRUG_COLUMN in df.columns else None\n",
    ")\n",
    "train_dataset.set_processed_texts(train_df[TEXT_COLUMN].apply(safe_preprocess).tolist())\n",
    "\n",
    "val_dataset = create_dataset_from_dataframe(\n",
    "    df=val_df,\n",
    "    text_column=TEXT_COLUMN,\n",
    "    label_column=LABEL_COLUMN,\n",
    "    drug_column=DRUG_COLUMN if DRUG_COLUMN in df.columns else None\n",
    ")\n",
    "val_dataset.set_processed_texts(val_df[TEXT_COLUMN].apply(safe_preprocess).tolist())\n",
    "\n",
    "test_dataset = create_dataset_from_dataframe(\n",
    "    df=test_df,\n",
    "    text_column=TEXT_COLUMN,\n",
    "    label_column=LABEL_COLUMN,\n",
    "    drug_column=DRUG_COLUMN if DRUG_COLUMN in df.columns else None\n",
    ")\n",
    "test_dataset.set_processed_texts(test_df[TEXT_COLUMN].apply(safe_preprocess).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6ec58",
   "metadata": {},
   "source": [
    "## 6. Using the Processed Data with Embeddings\n",
    "\n",
    "Once you have the preprocessed data, you can create embeddings. Here's an example with the shared embedding modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import get_embedding\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEAM-FINALIZED EMBEDDINGS: Creating all 3 for reference\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Embedding 1: Word2Vec Skip-gram (PRIMARY - Semantic understanding)\n",
    "print(\"\\nEmbedding 1: Word2Vec Skip-gram (PRIMARY)\")\n",
    "print(\"-\" * 70)\n",
    "w2v = get_embedding('word2vec', 'skipgram_medium')\n",
    "tokenizer = TextPreprocessor(lowercase=True, remove_punctuation=True, remove_stopwords=True)\n",
    "train_tokens = [tokenizer.tokenize(text) for text in train_dataset.get_processed()]\n",
    "w2v.fit(train_tokens)\n",
    "w2v_train_vectors = w2v.encode_texts(train_tokens)\n",
    "w2v_val_tokens = [tokenizer.tokenize(text) for text in val_dataset.get_processed()]\n",
    "w2v_val_vectors = w2v.encode_texts(w2v_val_tokens)\n",
    "print(f\"Word2Vec vectors shape: {w2v_train_vectors.shape}\")\n",
    "print(f\"Vocabulary size: {w2v.get_vocabulary_size()}\")\n",
    "print(f\"Use for: GRU, LSTM, RNN, Transformer models (best semantic understanding)\")\n",
    "\n",
    "# Embedding 2: GloVe (SECONDARY - Global context)\n",
    "print(\"\\nEmbedding 2: GloVe (SECONDARY)\")\n",
    "print(\"-\" * 70)\n",
    "glove = get_embedding('glove', 'medium')\n",
    "glove.fit(train_tokens)\n",
    "glove_train_vectors = glove.encode_texts(train_tokens)\n",
    "glove_val_vectors = glove.encode_texts(w2v_val_tokens)\n",
    "print(f\"GloVe vectors shape: {glove_train_vectors.shape}\")\n",
    "print(f\"Use for: Deep learning models (captures global patterns)\")\n",
    "\n",
    "# Embedding 3: TF-IDF (BASELINE - Statistical)\n",
    "print(\"\\nEmbedding 3: TF-IDF (BASELINE)\")\n",
    "print(\"-\" * 70)\n",
    "tfidf = get_embedding('tfidf', 'balanced')\n",
    "tfidf_train_vectors = tfidf.fit_transform(train_dataset.get_processed())\n",
    "tfidf_val_vectors = tfidf.transform_dense(val_dataset.get_processed())\n",
    "print(f\"TF-IDF vectors shape: {tfidf_train_vectors.shape}\")\n",
    "print(f\"Use for: Traditional ML and baseline comparisons (interpretable)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY: All 3 team embeddings reference created!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Create gru_model.ipynb (or your_model.ipynb)\")\n",
    "print(\"2. In that ONE notebook, implement your model with all 3 embeddings\")\n",
    "print(\"3. Compare results across all 3 embedding types in one place!\")\n",
    "print(\"\\nThis shared notebook shows you how to create and use each embedding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d88b8",
   "metadata": {},
   "source": [
    "## 7. How to Use This in Your Own Model Notebook\n",
    "\n",
    "Create ONE notebook per team member where you implement your model with ALL 3 embeddings. For example:\n",
    "\n",
    "**Essie (GRU)**: Create `gru_model.ipynb` with:\n",
    "- Section 1: Load and preprocess data (like this notebook)\n",
    "- Section 2: Build GRU model with Word2Vec Skip-gram embeddings + train + evaluate\n",
    "- Section 3: Build GRU model with GloVe embeddings + train + evaluate  \n",
    "- Section 4: Build GRU model with TF-IDF embeddings + train + evaluate\n",
    "- Section 5: Compare all 3 embedding results\n",
    "\n",
    "**Structure for your model notebook**:\n",
    "\n",
    "```python\n",
    "# At the top of your model notebook (e.g., gru_model.ipynb)\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Import shared utilities\n",
    "from src.preprocessing import get_preprocessor, TextPreprocessor\n",
    "from src.eda import EDAAnalyzer\n",
    "from src.data_utils import create_dataset_from_dataframe\n",
    "from embeddings import get_embedding\n",
    "\n",
    "# ====================================================================\n",
    "# SECTION 1: Load and Preprocess Data (same for all models)\n",
    "# ====================================================================\n",
    "DATA_PATH = '../data/drugLibTrain_raw.tsv'\n",
    "TEXT_COLUMN = 'commentsReview'\n",
    "LABEL_COLUMN = 'rating'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, sep='\\t')\n",
    "\n",
    "# Preprocess\n",
    "preprocessor = get_preprocessor('moderate')\n",
    "def safe_preprocess(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return preprocessor.process(str(text))\n",
    "\n",
    "processed_texts = df[TEXT_COLUMN].apply(safe_preprocess)\n",
    "\n",
    "# Split data\n",
    "train_df, val_df, test_df = train_test_split(df, test_size=0.2, \n",
    "                                              stratify=df[LABEL_COLUMN], \n",
    "                                              random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1/0.8, \n",
    "                                     stratify=train_df[LABEL_COLUMN], \n",
    "                                     random_state=42)\n",
    "\n",
    "# ====================================================================\n",
    "# SECTION 2: GRU Model with Word2Vec Skip-gram (PRIMARY)\n",
    "# ====================================================================\n",
    "print(\"Building GRU with Word2Vec Skip-gram embeddings...\")\n",
    "\n",
    "w2v = get_embedding('word2vec', 'skipgram_medium')\n",
    "tokenizer = TextPreprocessor(lowercase=True, remove_punctuation=True)\n",
    "train_tokens = [tokenizer.tokenize(text) for text in train_df[TEXT_COLUMN].apply(safe_preprocess)]\n",
    "w2v.fit(train_tokens)\n",
    "train_vectors = w2v.encode_texts(train_tokens)\n",
    "val_vectors = w2v.encode_texts([tokenizer.tokenize(text) for text in val_df[TEXT_COLUMN].apply(safe_preprocess)])\n",
    "test_vectors = w2v.encode_texts([tokenizer.tokenize(text) for text in test_df[TEXT_COLUMN].apply(safe_preprocess)])\n",
    "\n",
    "# Build and train GRU model with Word2Vec\n",
    "gru_w2v = models.Sequential([\n",
    "    layers.Input(shape=(w2v.embedding_dim,)),\n",
    "    layers.Reshape((1, w2v.embedding_dim)),\n",
    "    layers.GRU(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "gru_w2v.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gru_w2v.fit(train_vectors, train_df[LABEL_COLUMN].values, \n",
    "            validation_data=(val_vectors, val_df[LABEL_COLUMN].values),\n",
    "            epochs=10, batch_size=32, verbose=1)\n",
    "w2v_results = gru_w2v.evaluate(test_vectors, test_df[LABEL_COLUMN].values)\n",
    "\n",
    "# ====================================================================\n",
    "# SECTION 3: GRU Model with GloVe (SECONDARY)\n",
    "# ====================================================================\n",
    "print(\"Building GRU with GloVe embeddings...\")\n",
    "\n",
    "glove = get_embedding('glove', 'medium')\n",
    "glove.fit(train_tokens)\n",
    "train_vectors = glove.encode_texts(train_tokens)\n",
    "val_vectors = glove.encode_texts([tokenizer.tokenize(text) for text in val_df[TEXT_COLUMN].apply(safe_preprocess)])\n",
    "test_vectors = glove.encode_texts([tokenizer.tokenize(text) for text in test_df[TEXT_COLUMN].apply(safe_preprocess)])\n",
    "\n",
    "# Build and train GRU model with GloVe\n",
    "gru_glove = models.Sequential([\n",
    "    layers.Input(shape=(glove.embedding_dim,)),\n",
    "    layers.Reshape((1, glove.embedding_dim)),\n",
    "    layers.GRU(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "gru_glove.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gru_glove.fit(train_vectors, train_df[LABEL_COLUMN].values,\n",
    "              validation_data=(val_vectors, val_df[LABEL_COLUMN].values),\n",
    "              epochs=10, batch_size=32, verbose=1)\n",
    "glove_results = gru_glove.evaluate(test_vectors, test_df[LABEL_COLUMN].values)\n",
    "\n",
    "# ====================================================================\n",
    "# SECTION 4: GRU Model with TF-IDF (BASELINE)\n",
    "# ====================================================================\n",
    "print(\"Building GRU with TF-IDF embeddings...\")\n",
    "\n",
    "tfidf = get_embedding('tfidf', 'balanced')\n",
    "train_vectors = tfidf.fit_transform(train_df[TEXT_COLUMN].apply(safe_preprocess))\n",
    "val_vectors = tfidf.transform_dense(val_df[TEXT_COLUMN].apply(safe_preprocess))\n",
    "test_vectors = tfidf.transform_dense(test_df[TEXT_COLUMN].apply(safe_preprocess))\n",
    "\n",
    "# Build and train GRU model with TF-IDF\n",
    "gru_tfidf = models.Sequential([\n",
    "    layers.Input(shape=(tfidf.get_embedding_dim(),)),\n",
    "    layers.Reshape((1, tfidf.get_embedding_dim())),\n",
    "    layers.GRU(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "gru_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gru_tfidf.fit(train_vectors, train_df[LABEL_COLUMN].values,\n",
    "              validation_data=(val_vectors, val_df[LABEL_COLUMN].values),\n",
    "              epochs=10, batch_size=32, verbose=1)\n",
    "tfidf_results = gru_tfidf.evaluate(test_vectors, test_df[LABEL_COLUMN].values)\n",
    "\n",
    "# ====================================================================\n",
    "# SECTION 5: Compare Results\n",
    "# ====================================================================\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Embedding': ['Word2Vec Skip-gram', 'GloVe', 'TF-IDF'],\n",
    "    'Loss': [w2v_results[0], glove_results[0], tfidf_results[0]],\n",
    "    'Accuracy': [w2v_results[1], glove_results[1], tfidf_results[1]]\n",
    "})\n",
    "print(\"\\nGRU Model Results Comparison:\")\n",
    "print(results_df)\n",
    "```\n",
    "\n",
    "**Key Points for Your Implementation**:\n",
    "1. Load and preprocess data once at the beginning\n",
    "2. For each of 3 embeddings: create embedding, train GRU, evaluate\n",
    "3. Store results and compare at the end\n",
    "4. This single notebook contains everything for direct comparison\n",
    "\n",
    "**Expected notebook filename**: `gru_model.ipynb` (or similar for your model type)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
